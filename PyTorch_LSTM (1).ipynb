{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqTiCtZcTfmb",
        "colab_type": "code",
        "outputId": "7b61f9da-fa1d-4bd5-eaf9-58f9b86034c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "FOLDERNAME = 'ML/PyTorch_LSTM/data'\n",
        "\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "%cd drive/My\\ Drive\n",
        "%cp -r $FOLDERNAME ../../\n",
        "%cd ../../"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HE-cZ5k3IJl",
        "colab_type": "text"
      },
      "source": [
        "# This Notebook\n",
        "\n",
        "This notebook provides a fully PyTorch-based implementation of an LSTM at the nn.Module level of abstraction. The LSTM will be generic and word-level. \n",
        "\n",
        "Process:\n",
        "\n",
        "\n",
        "1.   Get the training data (x)\n",
        "2.   Encode the training data (remember we are encoding words using PyTorch embeddings)\n",
        "3.   Define the LSTM\n",
        "4.   Define the training loop\n",
        "5.   Define the predict function\n",
        "6.   Define the model saving function\n",
        "7.   Define the model loading function\n",
        "8.   Define the hyperparameters (x)\n",
        "9.   Define the sample method\n",
        "10.  Run the model\n",
        "11.  Define Dataset (x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy7x-wxIN83S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuxFZXoOfFmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Hyperparameters & Model Config \"\"\"\n",
        "\n",
        "model_name = \"LSTM_n_epoch_m_layer_l_seqlength_d_datasize\"\n",
        "data_file = \"headlines.txt\"\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "lr = 1e-3\n",
        "seq_length = 7\n",
        "num_layers = 3\n",
        "hidden_size = 512\n",
        "batch_size = 128\n",
        "dropout_prob = 0.5\n",
        "data_split = 0.2\n",
        "vector_size = 32\n",
        "num_epochs = 20\n",
        "print_every = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnYBJznnLn0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_file, batch_size=batch_size, seq_length=seq_length, transform=None):\n",
        "    self.data_file = data_file\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_length = seq_length\n",
        "    self.transform = transform\n",
        "    self.total_batch_size = batch_size * seq_length\n",
        "    \n",
        "    with open(self.data_file) as f:\n",
        "      self.x = f.read()\n",
        "\n",
        "  def __len__(self):\n",
        "    return sum(1 for line in open(self.data_file))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\" I'll be returning a matrix of dimensions (batch_size, seq_length)\"\"\"\n",
        "    batch_data = self.x[idx * self.total_batch_size : idx * self.total_batch_size + self.seq_length]\n",
        "    batch = batch_data.view(self.batch_size, self.seq_length)\n",
        "    return batch\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLQLtM6RN3SR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Incomplete - there are still issues with the forward pass \"\"\" \n",
        "class Word_Level_LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, bias, batch_first, dropout, data_split):\n",
        "    super(Word_Level_LSTM, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.bias = bias\n",
        "    self.batch_first = batch_first\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bias, batch_first, dropout)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\" Accepts a tensor of input data, produces a tensor of output data \"\"\"\n",
        "    # I have used variables that do not exist here \n",
        "    out, (h, c) = self.lstm(x, (h0, c0))    # out[dims] = [seq_len, batch, hidden_size]\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNV2ojcYT9Si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, x, y, num_epochs=num_epochs, learning_rate=lr, batch_size=batch_size, print_every=print_every):\n",
        "\n",
        "  if cuda: \n",
        "    torch.device('cuda')\n",
        "    print(\"Training with CUDA\")\n",
        "  \n",
        "  optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  dataset = Dataset(data_file)\n",
        "  step_count = 0\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "\n",
        "    for j in range(num_batches):\n",
        "\n",
        "      batch = dataset[j]\n",
        "      y_pred = model(batch)\n",
        "      loss = criterion(y_pred, batch)\n",
        "      optim.zero_grad()\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      step_count += 1\n",
        "\n",
        "      if step_count % print_every == 0:\n",
        "        # Give us loss statistics\n",
        "        \n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}